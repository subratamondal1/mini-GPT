{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bigram Language Model\n**Learns how words follow each other in sentences.**\n\n---","metadata":{}},{"cell_type":"markdown","source":"Download the Book **Wizard of OZ** as .txt file from [here.](https://github.com/subratamondal1/mini-GPT/raw/main/src/mini_gpt/data/wizard%20of%20oz.txt)","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\n\n# Hyperparameters\nblock_size = 8 # means that each training example will consist of a sequence of 8 consecutive tokens from input data\nbatch_size = 4 # each batch will contain 4 training examples (sequence of 8 consecutive tokens)\nlearning_rate = 3e-4\nmax_iterations = 1000\neval_iterations = 250\ndropout = 0.2","metadata":{"execution":{"iopub.status.busy":"2023-09-05T17:46:57.844284Z","iopub.execute_input":"2023-09-05T17:46:57.845117Z","iopub.status.idle":"2023-09-05T17:47:02.306476Z","shell.execute_reply.started":"2023-09-05T17:46:57.845029Z","shell.execute_reply":"2023-09-05T17:47:02.304808Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"! wget \"https://github.com/subratamondal1/mini-GPT/raw/main/src/mini_gpt/data/wizard%20of%20oz.txt\"","metadata":{"execution":{"iopub.status.busy":"2023-09-05T17:47:02.309108Z","iopub.execute_input":"2023-09-05T17:47:02.309724Z","iopub.status.idle":"2023-09-05T17:47:04.630361Z","shell.execute_reply.started":"2023-09-05T17:47:02.309674Z","shell.execute_reply":"2023-09-05T17:47:04.629290Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2023-09-05 17:47:03--  https://github.com/subratamondal1/mini-GPT/raw/main/src/mini_gpt/data/wizard%20of%20oz.txt\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/subratamondal1/mini-GPT/main/src/mini_gpt/data/wizard%20of%20oz.txt [following]\n--2023-09-05 17:47:03--  https://raw.githubusercontent.com/subratamondal1/mini-GPT/main/src/mini_gpt/data/wizard%20of%20oz.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 217429 (212K) [text/plain]\nSaving to: ‘wizard of oz.txt’\n\nwizard of oz.txt    100%[===================>] 212.33K  --.-KB/s    in 0.1s    \n\n2023-09-05 17:47:04 (1.80 MB/s) - ‘wizard of oz.txt’ saved [217429/217429]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"! tree /kaggle","metadata":{"execution":{"iopub.status.busy":"2023-09-05T17:47:04.631733Z","iopub.execute_input":"2023-09-05T17:47:04.632117Z","iopub.status.idle":"2023-09-05T17:47:05.751745Z","shell.execute_reply.started":"2023-09-05T17:47:04.632079Z","shell.execute_reply":"2023-09-05T17:47:05.750178Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[01;34m/kaggle\u001b[00m\n├── \u001b[01;34minput\u001b[00m\n├── \u001b[01;34mlib\u001b[00m\n│   └── \u001b[01;34mkaggle\u001b[00m\n│       └── gcp.py\n└── \u001b[01;34mworking\u001b[00m\n    └── wizard of oz.txt\n\n4 directories, 2 files\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Open the file **Wizard of OZ** which is in text form. ","metadata":{}},{"cell_type":"code","source":"with open(file = \"/kaggle/working/wizard of oz.txt\", mode = \"r\", encoding=\"utf-8\") as file:\n    text = file.read()\nprint(f\"Length of the text: {len(text)}.\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T17:47:05.755180Z","iopub.execute_input":"2023-09-05T17:47:05.755634Z","iopub.status.idle":"2023-09-05T17:47:05.766124Z","shell.execute_reply.started":"2023-09-05T17:47:05.755585Z","shell.execute_reply":"2023-09-05T17:47:05.764721Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Length of the text: 207797.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# First 200 texts\nprint(text[:200])","metadata":{"execution":{"iopub.status.busy":"2023-09-05T17:47:05.768219Z","iopub.execute_input":"2023-09-05T17:47:05.768747Z","iopub.status.idle":"2023-09-05T17:47:05.782383Z","shell.execute_reply.started":"2023-09-05T17:47:05.768712Z","shell.execute_reply":"2023-09-05T17:47:05.780882Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\nThe Wonderful Wizard of Oz\n\nby L. Frank Baum\n\n\nThis book is dedicated to my good friend & comrade\nMy Wife\nL.F.B.\n\n\nContents\n\n Introduction\n Chapter I. The Cyclone\n Chapter II. The Council with the Mu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Convert the texts into `characters`.","metadata":{}},{"cell_type":"code","source":"# Convert the texts into `unique characters`.\nunique_chars = sorted(set(text)) # Keeps only unique characters, and discard duplicate ones\nvocabulary_size = len(unique_chars)\nprint(f\"Set doesn't allow duplicates, hence the decreased : {len(text)} ---> {len(unique_chars)}\\n\\n{unique_chars}\")\nprint(f\"\\nVocabulary Size is {vocabulary_size}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T17:55:09.952922Z","iopub.execute_input":"2023-09-05T17:55:09.953331Z","iopub.status.idle":"2023-09-05T17:55:09.964462Z","shell.execute_reply.started":"2023-09-05T17:55:09.953279Z","shell.execute_reply":"2023-09-05T17:55:09.962843Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Set doesn't allow duplicates, hence the decreased : 207797 ---> 72\n\n['\\n', ' ', '!', '&', '(', ')', ',', '-', '.', '0', '1', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”']\n\nVocabulary Size is 72\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Character Tokenization\n**Character Level Tokenizer** converts the input text data into tokens of characters.","metadata":{}},{"cell_type":"code","source":"string_to_int = {char:index for index, char in enumerate(unique_chars)}\nint_to_string = {index:char for index, char in enumerate(unique_chars)}\n\n# Encoder & Decoder\nencode = lambda input_Text: [string_to_int[char] for char in input_Text]\ndecode = lambda encoded_Data: \"\".join([int_to_string[integer] for integer in encoded_Data])\n\nencoded_string = encode(\"SUBRATA MONDAL\")\ndecoded_string = decode(encode(\"SUBRATA MONDAL\"))\n\nprint(f\"Encoded Text:\\t {encoded_string}\\nDecoded Text:\\t {decoded_string}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T17:47:05.803697Z","iopub.execute_input":"2023-09-05T17:47:05.804201Z","iopub.status.idle":"2023-09-05T17:47:05.819909Z","shell.execute_reply.started":"2023-09-05T17:47:05.804155Z","shell.execute_reply":"2023-09-05T17:47:05.818051Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Encoded Text:\t [33, 35, 16, 32, 15, 34, 15, 1, 27, 29, 28, 18, 15, 26]\nDecoded Text:\t SUBRATA MONDAL\n","output_type":"stream"}]},{"cell_type":"code","source":"def character_tokenization(input_text:str, unique_chars):\n    string_to_int = {char:i for i, char in enumerate(unique_chars)}\n    int_to_string = {i:char for i, char in enumerate(unique_chars)}\n\n    # Encoder & Decoder\n    encode = lambda S: [string_to_int[c] for c in S]\n    decode = lambda L: \"\".join([int_to_string[i] for i in L])\n\n    encoded_string = encode(input_text)\n    decoded_string = decode(encode(input_text))\n    \n    return input_text, encoded_string, decoded_string","metadata":{"execution":{"iopub.status.busy":"2023-09-05T17:47:05.822189Z","iopub.execute_input":"2023-09-05T17:47:05.822856Z","iopub.status.idle":"2023-09-05T17:47:05.834527Z","shell.execute_reply.started":"2023-09-05T17:47:05.822791Z","shell.execute_reply":"2023-09-05T17:47:05.833024Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"character_tokens = character_tokenization(input_text=\"SUBRATA MONDAL\", unique_chars = unique_chars)\n\nprint(f\"Input Text:\\t {character_tokens[0]}\\nEncoded Text:\\t {character_tokens[1]}\\nDecoded Text:\\t {character_tokens[2]}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T17:47:05.836393Z","iopub.execute_input":"2023-09-05T17:47:05.836809Z","iopub.status.idle":"2023-09-05T17:47:05.856088Z","shell.execute_reply.started":"2023-09-05T17:47:05.836774Z","shell.execute_reply":"2023-09-05T17:47:05.854382Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Input Text:\t SUBRATA MONDAL\nEncoded Text:\t [33, 35, 16, 32, 15, 34, 15, 1, 27, 29, 28, 18, 15, 26]\nDecoded Text:\t SUBRATA MONDAL\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Convert the whole text data into PyTorch Tensor.\n* First Encode into Integers, then Convert to Torch Tensor.","metadata":{}},{"cell_type":"code","source":"data = torch.tensor(data=encode(text), dtype=torch.long)\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-05T17:47:05.857577Z","iopub.execute_input":"2023-09-05T17:47:05.858055Z","iopub.status.idle":"2023-09-05T17:47:05.957357Z","shell.execute_reply.started":"2023-09-05T17:47:05.858008Z","shell.execute_reply":"2023-09-05T17:47:05.956210Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"torch.Size([207797])"},"metadata":{}}]},{"cell_type":"markdown","source":"How Bigram Language Models work at a fundamental level.\n* Bigram Language Model predicts the next word based on the previous words. It is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2.","metadata":{}},{"cell_type":"code","source":"# How Bigram Language Models work at a fundamental level.\nprediction_X = data[:block_size] \ntarget_y = data[1:block_size+1]\n\nfor t in range(block_size):\n    input_text = prediction_X[:t+1]\n    target = target_y[t]\n    print(f\"When Input is {input_text} then target is {target}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T17:47:05.962227Z","iopub.execute_input":"2023-09-05T17:47:05.962635Z","iopub.status.idle":"2023-09-05T17:47:06.018004Z","shell.execute_reply.started":"2023-09-05T17:47:05.962602Z","shell.execute_reply":"2023-09-05T17:47:06.016807Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"When Input is tensor([0]) then target is 34\nWhen Input is tensor([ 0, 34]) then target is 48\nWhen Input is tensor([ 0, 34, 48]) then target is 45\nWhen Input is tensor([ 0, 34, 48, 45]) then target is 1\nWhen Input is tensor([ 0, 34, 48, 45,  1]) then target is 37\nWhen Input is tensor([ 0, 34, 48, 45,  1, 37]) then target is 55\nWhen Input is tensor([ 0, 34, 48, 45,  1, 37, 55]) then target is 54\nWhen Input is tensor([ 0, 34, 48, 45,  1, 37, 55, 54]) then target is 44\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train Validation Split","metadata":{}},{"cell_type":"code","source":"n = int(0.8 * len(data))\nprint(f\"n is {n}\")\n\ntrain_data = data[:n]\nval_data = data[n:]\n\n# generate batches of training or validation data\ndef get_batch(split): \n    data = train_data if split == \"train\" else val_data\n    ix = torch.randint(high=len(data) - block_size, size = (batch_size,))\n    \n    x = torch.stack(tensors=[ data[i: i+block_size] for i in ix])\n    y = torch.stack(tensors=[ data[i+1: i+block_size+1] for i in ix])\n    return x.to(device), y.to(device) # move x,y to gpu if available\n\n# x is inputs, y is target or label\nx,y = get_batch(\"train\")\nprint(f\"\\ninputs --- {x.shape} --- \\n{x}\\n\")\nprint(f\"targets --- {y.shape} --- \\n{y}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T18:29:43.743860Z","iopub.execute_input":"2023-09-05T18:29:43.744372Z","iopub.status.idle":"2023-09-05T18:29:43.758675Z","shell.execute_reply.started":"2023-09-05T18:29:43.744331Z","shell.execute_reply":"2023-09-05T18:29:43.757197Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"n is 166237\n\ninputs --- torch.Size([4, 8]) --- \ntensor([[53, 54,  6,  1, 41, 54, 44,  1],\n        [ 1, 27, 61, 54, 43, 48, 51, 49],\n        [ 1, 58, 55, 55, 53,  8,  1, 23],\n        [61,  1, 47, 55,  1, 23,  1, 43]])\n\ntargets --- torch.Size([4, 8]) --- \ntensor([[54,  6,  1, 41, 54, 44,  1, 58],\n        [27, 61, 54, 43, 48, 51, 49, 54],\n        [58, 55, 55, 53,  8,  1, 23, 60],\n        [ 1, 47, 55,  1, 23,  1, 43, 45]])\n\n","output_type":"stream"}]},{"cell_type":"code","source":"@torch.no_grad() # reduces memory usage, improves performance as we are not calculating the gradient/slope\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in [\"train\",\"val\"]:\n        losses = torch.zeros(eval_iterations)\n        for k in range(eval_iterations):\n            X,y = get_batch(split)\n            logits, loss = model(X,y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2023-09-05T17:47:06.066249Z","iopub.execute_input":"2023-09-05T17:47:06.066722Z","iopub.status.idle":"2023-09-05T17:47:06.073778Z","shell.execute_reply.started":"2023-09-05T17:47:06.066690Z","shell.execute_reply":"2023-09-05T17:47:06.072888Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network\n---","metadata":{"execution":{"iopub.status.busy":"2023-09-02T05:24:11.638016Z","iopub.execute_input":"2023-09-02T05:24:11.638470Z","iopub.status.idle":"2023-09-02T05:24:11.647782Z","shell.execute_reply.started":"2023-09-02T05:24:11.638433Z","shell.execute_reply":"2023-09-02T05:24:11.646137Z"}}},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.nn import functional as F","metadata":{"execution":{"iopub.status.busy":"2023-09-05T17:47:06.075292Z","iopub.execute_input":"2023-09-05T17:47:06.075663Z","iopub.status.idle":"2023-09-05T17:47:06.086979Z","shell.execute_reply.started":"2023-09-05T17:47:06.075623Z","shell.execute_reply":"2023-09-05T17:47:06.085835Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Define a Bigram Language Model using PyTorch\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocabulary_size):\n        super(BigramLanguageModel, self).__init__()\n        # Create an embedding table to convert token indices into dense vectors\n        self.tokens_embedding_table = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=vocabulary_size)\n        \n    def forward(self, input_tokens, targets=None):\n        # Calculate logits (scores) for input tokens using the embedding table\n        logits = self.tokens_embedding_table(input_tokens)\n        \n        if targets is None:\n            loss = None\n        else:\n            # Reshape logits and targets for cross-entropy loss calculation\n            batch_size, tokens_in_a_sequence, vocabulary_size = logits.shape\n            logits = logits.view(batch_size * tokens_in_a_sequence, vocabulary_size)\n            targets = targets.view(batch_size * tokens_in_a_sequence)\n            # Calculate cross-entropy loss\n            loss = F.cross_entropy(input=logits, target=targets)\n        \n        return logits, loss\n    \n    def generate(self, input_tokens, max_new_tokens):\n        for _ in range(max_new_tokens):\n            # Get predictions and loss using the forward method\n            logits, loss = self.forward(input_tokens)\n            # Focus on the last token in the sequence\n            logits = logits[:, -1, :]  # Shape: (batch_size, vocabulary_size)\n            # Apply softmax to get token probabilities\n            probabilities = F.softmax(input=logits, dim=-1)  # Shape: (batch_size, vocabulary_size)\n            # Sample the next token from the distribution\n            input_tokens_next = torch.multinomial(input=probabilities, num_samples=1)  # Shape: (batch_size, 1)\n            # Append the sampled token to the running sequence\n            input_tokens = torch.cat((input_tokens, input_tokens_next), dim=1)  # Shape: (batch_size, tokens_in_a_sequence + 1)\n            \n        return input_tokens\n\n# Create an instance of the Bigram Language Model\nmodel = BigramLanguageModel(vocabulary_size=vocabulary_size)\nm = model.to(device)\n\n# Create an initial input token (e.g., start of a sequence)\ninput_tokens = torch.zeros(size=(1, 1), dtype=torch.long, device=device)\n\n# Generate a sequence of tokens using the Bigram Language Model\ngenerated_tokens = m.generate(input_tokens=input_tokens, max_new_tokens=500)\n\n# Convert generated token indices to text using a decoding function\ngenerated_text = decode(generated_tokens[0].tolist())\n\n# Print the generated text\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T18:42:37.202933Z","iopub.execute_input":"2023-09-05T18:42:37.203338Z","iopub.status.idle":"2023-09-05T18:42:37.305652Z","shell.execute_reply.started":"2023-09-05T18:42:37.203290Z","shell.execute_reply":"2023-09-05T18:42:37.304461Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"\nLXHkgdDMHSxZ(YC;FSYV“YVdnvKeuGoBUH!wa&goK !cMl mpvV—&fjffnvW—;u?fD::Ovej“-O.Q\nwe;iOA-Hh?MlOlrjUYLhEgW0:CwAOAmIKOTLXEuMh\nZs;XhTaE\ngzfHuy,eHUjcQd1,ZyZteuG“GR.uVdNsQFinfn1:zzUwuObr?xUsQ-”XrcWeNVsbvaP:Dt)HtnWqv1&KkQW“-nNZt.yMljjHxLK0QZWcknwLXvApbQV“ eYnYnvCdk““pXJ\nvZsjHXA9.\nn1m.9hjEglDNTgUVRCLevIWqY(!oMh?VahPK?QFSpjhz9xKH9”yZZtqQpJk”f\n?SbS”IwN;rBZsTynVKzyIwi&AaAp,KzTnMleO“,K0fEao1WcWsnbLKdR“uL9AXA.CdN\n?—NOrv0NC:ULck—KDpj1x?oaoa,vL0eOi,“U(FS 9-ANZyPW-ub1lOKadg—H.VLmN\n1SfU&Y!nKzW0DtfU&tKr)QWDssEYPC”YB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Optimizer\n---","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\noptimizer","metadata":{"execution":{"iopub.status.busy":"2023-09-05T18:42:50.897805Z","iopub.execute_input":"2023-09-05T18:42:50.898179Z","iopub.status.idle":"2023-09-05T18:42:50.906832Z","shell.execute_reply.started":"2023-09-05T18:42:50.898151Z","shell.execute_reply":"2023-09-05T18:42:50.905379Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"AdamW (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0.01\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Training Loop\n---","metadata":{}},{"cell_type":"code","source":"for iter in range(max_iterations):\n    if iter % eval_iterations == 0:\n        # Estimate and print the training and validation losses\n        losses = estimate_loss()\n        print(f\"Step is {iter}, Train Loss is {losses['train']:.4f}, Val Loss is {losses['val']:.4f}\")\n    \n    # Get a batch of training data (input and target)\n    xb, yb = get_batch(\"train\")\n    \n    # Forward pass: Calculate logits (scores) and loss for the batch using the model\n    logits, loss = model.forward(input_tokens=xb, targets=yb)\n    \n    # Backpropagation: Zero the gradients, perform backpropagation, and update the model's parameters\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# After training iterations are completed, print the final loss\nprint(f\"Loss is {loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T18:45:31.623145Z","iopub.execute_input":"2023-09-05T18:45:31.623585Z","iopub.status.idle":"2023-09-05T18:45:32.899590Z","shell.execute_reply.started":"2023-09-05T18:45:31.623550Z","shell.execute_reply":"2023-09-05T18:45:32.898260Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Step is 0, Train Loss is 4.7941, Val Loss is 4.7787\nStep is 250, Train Loss is 4.7368, Val Loss is 4.7234\nStep is 500, Train Loss is 4.6710, Val Loss is 4.6709\nStep is 750, Train Loss is 4.6006, Val Loss is 4.6069\nLoss is 4.562045574188232\n","output_type":"stream"}]},{"cell_type":"code","source":"input_tokens = torch.zeros(size=(1,1), dtype=torch.long, device=device)\ngenerated_charts = decode(m.generate(input_tokens = input_tokens, max_new_tokens=500)[0].tolist())\nprint(generated_charts)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T18:44:21.489224Z","iopub.execute_input":"2023-09-05T18:44:21.489708Z","iopub.status.idle":"2023-09-05T18:44:21.569791Z","shell.execute_reply.started":"2023-09-05T18:44:21.489671Z","shell.execute_reply":"2023-09-05T18:44:21.568557Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"\n.Km.x(P\nfs(v“’vV’jKzUWd“zUtZ dN?ogG:cdCmfCrbi&ggtgs1pxPH,U,L\n1fbj,Kl:w1svRCvk;lu—qC(JnvNhej,u JKeQ0!G”XRSzXEZcvWs1k(u rF—x0SvZSff“p1KdaSNvHj“:;?xbwv);-XFFJo )FXFgpLMV\nh-BQW \nwLJZMeVG1xPbbsVAl9W-&fbKQrqy0qvCD‘.YLX:hV”uqLDtxRS‘)FSKHpx\nJn0:!cWoLMe\nJUa?y“:?yUMJU0:rowvev“’hrJ’UxrhZrquGS‘!9;L0KfPDkd&bp JwT“PEE”YoiizU&CdNsapB.9”L;ynv””u&Du&R1;wt&pP“QFTc“g—nD.vnveT?TdRTn1WvnNTdAJBbZL9DMf uGA..1apjUXHreNMxxAbiVEfap :-CCCeYl:9AeSLfM)trji1fqn;ArX!W0k“““KNxuuRSzErg.uOse0jVkYsYNZW0ni”WRueXE9bX”RSsxoMUnHZSODc\n","output_type":"stream"}]},{"cell_type":"markdown","source":"---\n# Mini GPT\n**Character Level Language Model**\n\n---","metadata":{}},{"cell_type":"code","source":"!wget \"https://github.com/karpathy/char-rnn/raw/master/data/tinyshakespeare/input.txt\"","metadata":{"execution":{"iopub.status.busy":"2023-09-05T18:56:13.500324Z","iopub.execute_input":"2023-09-05T18:56:13.500971Z","iopub.status.idle":"2023-09-05T18:56:15.665443Z","shell.execute_reply.started":"2023-09-05T18:56:13.500925Z","shell.execute_reply":"2023-09-05T18:56:15.663607Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"--2023-09-05 18:56:14--  https://github.com/karpathy/char-rnn/raw/master/data/tinyshakespeare/input.txt\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt [following]\n--2023-09-05 18:56:14--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt’\n\ninput.txt           100%[===================>]   1.06M  5.51MB/s    in 0.2s    \n\n2023-09-05 18:56:15 (5.51 MB/s) - ‘input.txt’ saved [1115394/1115394]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!tree /kaggle","metadata":{"execution":{"iopub.status.busy":"2023-09-05T18:56:40.600196Z","iopub.execute_input":"2023-09-05T18:56:40.600689Z","iopub.status.idle":"2023-09-05T18:56:41.708914Z","shell.execute_reply.started":"2023-09-05T18:56:40.600643Z","shell.execute_reply":"2023-09-05T18:56:41.707448Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"\u001b[01;34m/kaggle\u001b[00m\n├── \u001b[01;34minput\u001b[00m\n├── \u001b[01;34mlib\u001b[00m\n│   └── \u001b[01;34mkaggle\u001b[00m\n│       └── gcp.py\n└── \u001b[01;34mworking\u001b[00m\n    ├── input.txt\n    └── wizard of oz.txt\n\n4 directories, 3 files\n","output_type":"stream"}]},{"cell_type":"code","source":"data_path = \"/kaggle/working/input.txt\"","metadata":{"execution":{"iopub.status.busy":"2023-09-05T18:59:33.316472Z","iopub.execute_input":"2023-09-05T18:59:33.316981Z","iopub.status.idle":"2023-09-05T18:59:33.323968Z","shell.execute_reply.started":"2023-09-05T18:59:33.316937Z","shell.execute_reply":"2023-09-05T18:59:33.322221Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"with open(file=data_path, mode=\"r\", encoding=\"utf-8\") as f:\n    text = f.read()","metadata":{"execution":{"iopub.status.busy":"2023-09-05T19:21:12.586061Z","iopub.execute_input":"2023-09-05T19:21:12.586504Z","iopub.status.idle":"2023-09-05T19:21:12.594186Z","shell.execute_reply.started":"2023-09-05T19:21:12.586470Z","shell.execute_reply":"2023-09-05T19:21:12.593013Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# length\nprint(f\"Length of the dataset in characters: {len(text)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T19:21:52.689959Z","iopub.execute_input":"2023-09-05T19:21:52.690922Z","iopub.status.idle":"2023-09-05T19:21:52.697724Z","shell.execute_reply.started":"2023-09-05T19:21:52.690872Z","shell.execute_reply":"2023-09-05T19:21:52.696400Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Length of the dataset in characters: 1115394\n","output_type":"stream"}]},{"cell_type":"code","source":"# text\nprint(text[:1000])","metadata":{"execution":{"iopub.status.busy":"2023-09-05T20:00:33.372401Z","iopub.execute_input":"2023-09-05T20:00:33.372866Z","iopub.status.idle":"2023-09-05T20:00:33.379651Z","shell.execute_reply.started":"2023-09-05T20:00:33.372835Z","shell.execute_reply":"2023-09-05T20:00:33.378272Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Vocabulary","metadata":{}},{"cell_type":"code","source":"# here are all the unique characters, \nunique_chars = sorted(list(set(text))) \nvocab_size = len(unique_chars)\n\nprint(f\"The Vocabulary Size is {vocab_size}, it's way less than 1.1 Million because, Vocabulary Size contains only unique characters.\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T19:36:34.524144Z","iopub.execute_input":"2023-09-05T19:36:34.524570Z","iopub.status.idle":"2023-09-05T19:36:34.552590Z","shell.execute_reply.started":"2023-09-05T19:36:34.524536Z","shell.execute_reply":"2023-09-05T19:36:34.551068Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"The Vocabulary Size is 65, it's way less than 1.1 Million because, Vocabulary Size contains only unique characters.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenizer\n\nSimple Tokenizer using Encoding and Decoding.","metadata":{}},{"cell_type":"code","source":"# map characters to integers\nstring_to_int = {char:index for index, char in enumerate(unique_chars)}\n# map int (encoded string) back to text string\nint_to_string = {index:char for index, char in enumerate(unique_chars)}\n\n# Encoder converts from text string to int\nencode = lambda input_Text: [string_to_int[char] for char in input_Text]\n# Decoder converts Encoded string (int) back to text string\ndecode = lambda encoded_Data: \"\".join([int_to_string[integer] for integer in encoded_Data])\n\nencoded_string = encode(\"SUBRATA MONDAL\")\ndecoded_string = decode(encode(\"SUBRATA MONDAL\"))\n\nprint(f\"Encoded Text:\\t {encoded_string}\\nDecoded Text:\\t {decoded_string}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T19:42:06.458168Z","iopub.execute_input":"2023-09-05T19:42:06.458640Z","iopub.status.idle":"2023-09-05T19:42:06.468001Z","shell.execute_reply.started":"2023-09-05T19:42:06.458606Z","shell.execute_reply":"2023-09-05T19:42:06.466454Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Encoded Text:\t [31, 33, 14, 30, 13, 32, 13, 1, 25, 27, 26, 16, 13, 24]\nDecoded Text:\t SUBRATA MONDAL\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Note that:\n* OpenAi uses [tiktoken](https://github.com/openai/tiktoken) tokenizer.\n* Google uses [sentencepiece](https://github.com/google/sentencepiece) tokenizer.","metadata":{}},{"cell_type":"code","source":"# Let's now encode the entire text dataset and store it in torch.Tensor\nimport torch\ndata = torch.tensor(encode(text),dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # let's see the first 1000 characters","metadata":{"execution":{"iopub.status.busy":"2023-09-05T20:01:16.022253Z","iopub.execute_input":"2023-09-05T20:01:16.022754Z","iopub.status.idle":"2023-09-05T20:01:16.333833Z","shell.execute_reply.started":"2023-09-05T20:01:16.022718Z","shell.execute_reply":"2023-09-05T20:01:16.332542Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"torch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}